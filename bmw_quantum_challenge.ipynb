{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a5f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cf38e",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58e2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6236e7a",
   "metadata": {},
   "source": [
    "### load data from S3 to folder of sagemaker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfd79581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# When running on SageMaker, need execution role\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Declare bucket name, remote file, and destination\n",
    "s3_bucket = \"amazon-braket-a79a20b5e0b4\"\n",
    "orig_file = 'defect_data.zip'\n",
    "dest_file = './defect_data.zip'\n",
    "\n",
    "# Connect to S3 bucket and download file\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(s3_bucket).download_file(orig_file, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11368a99",
   "metadata": {},
   "source": [
    "### unzip defect_data.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Create a ZipFile Object and load sample.zip in it\n",
    "with ZipFile('defect_data.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall('./defect_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc38c14",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f322a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(img_path, img_set_size = 40e3, test_set_ratio = 0.2):\n",
    "    \"\"\"\n",
    "    Create train and test data with PyTorchs ImageFolder class.  \n",
    "    \"\"\"\n",
    "\n",
    "    resolution = (227,227)\n",
    "    \n",
    "    img_dataset = datasets.ImageFolder(\n",
    "        root=img_path,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(resolution), # do not have to resize because all imgs are on 227x227\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "        ]))\n",
    "    \n",
    "    # restrict size of data set\n",
    "    # take n random indices out of full img_dataset size\n",
    "    random_indices = random.sample(range(len(img_dataset)), img_set_size)\n",
    "\n",
    "    img_dataset_subset = torch.utils.data.Subset(img_dataset, random_indices)\n",
    "    \n",
    "    # split data set into train and test set\n",
    "    test_set_size = int(test_set_ratio * len(img_dataset_subset))\n",
    "    train_set_size = len(img_dataset_subset) - test_set_size   \n",
    "    train_set, test_set = random_split(img_dataset_subset, [train_set_size, test_set_size], generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    # with attribute class_to_idx we get a mapping from classes to labels. We invert the dict subsequently.\n",
    "    label_to_class_dict = {value: key for key, value in img_dataset.class_to_idx.items()}\n",
    "\n",
    "    return train_set, test_set, label_to_class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae9767",
   "metadata": {},
   "source": [
    "### load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2fa8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './defect_data'\n",
    "train_set, test_set, label_to_class_dict = get_train_test_data(img_path, img_set_size = 1000)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299f08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48286020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f67034",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3dda0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_accuracy(y_pred, y_true):\n",
    "    return (y_pred.argmax(dim=1) == y_true).float().mean()\n",
    "\n",
    "def eval_test_set(model, best_loss, model_name, run_idx):\n",
    "    print(\"### Eval test set ###\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction = 'mean')    \n",
    "        \n",
    "    y_test_pred = torch.tensor([]).float()\n",
    "    y_test_true = torch.tensor([]).long()  \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # forward pass is faster with torch.no_grad()\n",
    "    with torch.no_grad(): \n",
    "            \n",
    "        for x_test_batch, y_test_true_batch in test_loader:\n",
    "            \n",
    "            y_test_pred_batch = model(x_test_batch)\n",
    "            \n",
    "            y_test_pred = torch.cat([y_test_pred, y_test_pred_batch])\n",
    "            y_test_true = torch.cat([y_test_true, y_test_true_batch])     \n",
    "\n",
    "    loss_test = loss_fn(y_test_pred, y_test_true)\n",
    "\n",
    "    acc_test = get_plain_accuracy(y_test_pred, y_test_true)\n",
    "\n",
    "    print(f\"Loss test: {loss_test:.3f}\")\n",
    "    print(f\"Accuracy test: {acc_test:.3f}\") \n",
    "    print(f'Time {(time.time() - start_time):.2f}')\n",
    "\n",
    "    # save best_model\n",
    "    if loss_test < best_loss:\n",
    "        state = {\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        torch.save(state, f'./models/{model_name}_best_model_run_{run_idx}.h5')\n",
    "        best_loss = loss_test\n",
    "        \n",
    "    print(\"#####################\\n\")    \n",
    "    return best_loss, loss_test, acc_test  \n",
    "\n",
    "\n",
    "def train(model, num_epochs, run_idx, model_name=\"resnet\"):\n",
    "    \"\"\"Train function\"\"\"\n",
    "    \n",
    "\n",
    "    # define loss and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction = 'mean')\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    acc_train_list = []\n",
    "    loss_train_list = []    \n",
    "    \n",
    "    acc_test_list = []    \n",
    "    loss_test_list = []   \n",
    "    \n",
    "    best_loss = 10e3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f'### Epoch {epoch} ###\\n')\n",
    "\n",
    "        for idx, (x_train, y_true) in enumerate(train_loader):\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(f'Batch iter/total {idx}/{len(train_loader)-1}')\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do one forward pass\n",
    "            y_pred = model(x_train)   \n",
    "                            \n",
    "            loss = loss_fn(y_pred, y_true)           \n",
    "                \n",
    "            acc_train = get_plain_accuracy(y_pred, y_true)\n",
    "\n",
    "            print(f\"Loss train: {loss:.3f}\")\n",
    "            print(f\"Accuracy train: {acc_train:.3f}\")\n",
    "                \n",
    "            # get gradients with respect to that loss\n",
    "            loss.backward()    \n",
    "                       \n",
    "            # actual optimizing step\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_train_list.append(acc_train.item())\n",
    "            loss_train_list.append(loss.item())\n",
    "            \n",
    "            print(f'Time per batch_iter {(time.time() - start_time):.2f}\\n')\n",
    "        \n",
    "            # eval test set every 5 iters\n",
    "            if ((idx + 1) % 5) == 0:\n",
    "                best_loss, loss_test, acc_test = eval_test_set(model, best_loss, model_name,run_idx)\n",
    "                \n",
    "                acc_test_list.append(acc_test.item())\n",
    "                loss_test_list.append(loss_test.item())\n",
    "                \n",
    "    return acc_train_list, acc_test_list, loss_train_list, loss_test_list\n",
    "\n",
    "\n",
    "def plot_stuff(data_, title, plot_each_metric=False):\n",
    "    \n",
    "    data = data_.mean(axis=0)\n",
    "    std = data_.std(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.title(title)\n",
    "    plt.plot(data)\n",
    "    \n",
    "    if plot_each_metric:\n",
    "        plt.plot(data_.T)\n",
    "    \n",
    "    plt.fill_between(range(len(data)), data - std, data + std, color='tab:blue', alpha=0.15)\n",
    "                 \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f933359",
   "metadata": {},
   "source": [
    "### load pretrained resnet 18 and freeze all conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abb0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(num_runs, num_epochs, nn_type, q_depth=None):\n",
    "    # start training process and measure time duration\n",
    "    start_time = time.time()\n",
    "\n",
    "    avg_acc_train = np.array([])\n",
    "    avg_acc_test = np.array([])\n",
    "\n",
    "    avg_loss_train = np.array([])\n",
    "    avg_loss_test = np.array([])\n",
    "\n",
    "    for run_idx in range(num_runs):\n",
    "\n",
    "        # load on ImageNet pretrained resnet18\n",
    "        model = models.resnet18(pretrained=True)\n",
    "\n",
    "        # freeze all layers\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # attach new last classic or quantum layer\n",
    "        if nn_type == 'classic':\n",
    "            model.fc = nn.Linear(512, 2)  \n",
    "            model_name=\"resnet\"\n",
    "            \n",
    "        elif nn_type == 'angular':\n",
    "            model.fc = QuantumNetAngularEmbedding(n_qubits=4, q_depth=q_depth)\n",
    "            model_name=f\"resnet_q_angular_depth{q_depth}\"\n",
    "            \n",
    "        elif nn_type == 'amplitude':\n",
    "            model.fc = QuantumNetAmplitudeEmbedding(n_qubits=9, q_depth=q_depth)\n",
    "            model_name=f\"resnet_q_amplitude_depth{q_depth}\"    \n",
    "            \n",
    "        if  run_idx == 0:  \n",
    "            print(f'########## {model_name} ##########')\n",
    "            \n",
    "        print(f'\\n########## run {run_idx} ##########\\n')\n",
    "\n",
    "        acc_train, acc_test, loss_train, loss_test = train(model=model, num_epochs = num_epochs, run_idx= run_idx,model_name=model_name)\n",
    "\n",
    "        avg_acc_train = np.vstack([avg_acc_train, np.array(acc_train)]) if avg_acc_train.size else np.array(acc_train)  \n",
    "        avg_acc_test = np.vstack([avg_acc_test, np.array(acc_test)]) if avg_acc_test.size else np.array(acc_test)  \n",
    "\n",
    "        avg_loss_train = np.vstack([avg_loss_train, np.array(loss_train)]) if avg_loss_train.size else np.array(loss_train)  \n",
    "        avg_loss_test = np.vstack([avg_loss_test, np.array(loss_test)]) if avg_loss_test.size else np.array(loss_test)        \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    print(f'total time taken {total_time:.2f}\\n')\n",
    "    \n",
    "    \n",
    "    # create folder if not exists\n",
    "    pathlib.Path(f\"./metrics/{model_name}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # store metrics\n",
    "    np.save(f'./metrics/{model_name}/avg_acc_test.npy', avg_acc_test)\n",
    "    np.save(f'./metrics/{model_name}/avg_acc_train.npy', avg_acc_train)\n",
    "\n",
    "    np.save(f'./metrics/{model_name}/avg_loss_train.npy', avg_loss_train)\n",
    "    np.save(f'./metrics/{model_name}/avg_loss_test.npy', avg_loss_test)\n",
    "    \n",
    "    return avg_acc_train.T, avg_acc_test.T, avg_loss_train.T, avg_loss_test.T\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea389b34",
   "metadata": {},
   "source": [
    "## Train CNN classically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a8309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 3\n",
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde6be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## resnet ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.741\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 0.38\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.660\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 0.29\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.656\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 0.28\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.566\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 0.27\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 0.27\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.624\n",
      "Accuracy test: 0.670\n",
      "Time 2.19\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b7f73ed2e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mavg_acc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_acc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'classic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9857f445b80d>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(num_runs, num_epochs, nn_type, q_depth)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n########## run {run_idx} ##########\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_idx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mavg_acc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavg_acc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mavg_acc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d1c82388b298>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, run_idx, model_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# do one forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Braket/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \n",
    "avg_acc_train, avg_acc_test, avg_loss_train, avg_loss_test = run_experiment(num_runs, num_epochs, nn_type='classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb221b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max average test accuracy 0.992\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "avg_acc_test = np.load('./metrics/resnet/avg_acc_test.npy')\n",
    "avg_acc_train = np.load('./metrics/resnet/avg_acc_train.npy')\n",
    "\n",
    "avg_loss_train = np.load('./metrics/resnet/avg_loss_train.npy')\n",
    "avg_loss_test = np.load('./metrics/resnet/avg_loss_test.npy')\n",
    "\n",
    "print(f'max average test accuracy {max(avg_acc_test.mean(axis=0)):.3f}')\n",
    "\n",
    "plot_stuff(avg_acc_test, title = 'Acc test')\n",
    "plot_stuff(avg_acc_train, title = 'Acc train')\n",
    "\n",
    "plot_stuff(avg_loss_train, title = 'Loss train')\n",
    "plot_stuff(avg_loss_test, title = 'Loss test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147c3a2",
   "metadata": {},
   "source": [
    "### Quantum Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f997b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "n_qubits = 4        \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81d4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entangling_layer(n_qubits, pattern):\n",
    "    \"\"\"pattern in ('chain', 'ring', 'all_to_all')\"\"\"\n",
    "    \n",
    "    if pattern not in ('chain', 'ring', 'all_to_all', 'random'):\n",
    "        pattern = 'chain'\n",
    "    if pattern is not 'random':\n",
    "        qml.broadcast(qml.CNOT, wires = list(range(n_qubits)), pattern = pattern)\n",
    "    else:\n",
    "        q_idx_list = list(range(n_qubits))\n",
    "        np.random.shuffle(q_idx_list)\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires = [q_idx_list[i], q_idx_list[i + 1]])\n",
    "        qml.CNOT(wires = [q_idx_list[-1], q_idx_list[0]])\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "        \n",
    "def RX_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the x axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RX(element, wires=idx)\n",
    "\n",
    "def amplitude_embedding(q_input_features, n_qubits):\n",
    "    \"\"\"Transfers a given input feature into a quantum statevector. 2**9 = 512\"\"\"\n",
    "\n",
    "    \n",
    "    assert len(q_input_features) == 2**n_qubits, \"Input Features must be of dimension 2**n_qubits\"\n",
    "    \n",
    "    wires_list = list(range(n_qubits))\n",
    "    \n",
    "    # TODO: replace equality comparison to one with tolerance\n",
    "    normalize = not (np.linalg.norm(q_input_features) == 1)\n",
    "            \n",
    "    qml.templates.AmplitudeEmbedding(features = q_input_features, \n",
    "                                     wires = wires_list, \n",
    "                                     normalize = normalize) \n",
    "        \n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        RX_layer(q_weights[k])        \n",
    "        RY_layer(q_weights[k])\n",
    "        \n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "        \n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    \n",
    "    return tuple(exp_vals)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7e14f",
   "metadata": {},
   "source": [
    "#### draw circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6002211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: HRY(-0.215)RX(-0.249)RY(-0.249)CX Z \n",
      " 1: HRY(0.203)RX(0.587)RY(0.587)XC Z \n",
      " 2: HRY(0.26)RX(-1.65)RY(-1.65)XC Z \n",
      " 3: HRY(-1.12)RX(-1.61)RY(-1.61)XC Z \n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_depth = 1\n",
    "n_qubits = 4\n",
    "\n",
    "q_input_features = (torch.rand(n_qubits) * 2 - 1) * np.pi / 2\n",
    "q_weights_flat = torch.randn(q_depth * n_qubits)\n",
    "\n",
    "# evaluation of the circuit is necessary to draw the circuit \n",
    "result = quantum_net(q_weights_flat, q_input_features)\n",
    "\n",
    "print(quantum_net.draw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935d3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = \"amazon-braket-a79a20b5e0b4\"  # the name of the bucket\n",
    "my_prefix = \"amplitude-embbing\"  # the name of the folder in the bucket\n",
    "s3_folder = (my_bucket, my_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6208aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNetAngularEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,n_qubits, q_depth):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(512, n_qubits)\n",
    "        init_param_spread = 0.01\n",
    "        self.q_params = nn.Parameter(init_param_spread * torch.randn(q_depth * n_qubits))\n",
    "        print('in init self.q_params',self.q_params)\n",
    "        self.post_net = nn.Linear(n_qubits, 2)\n",
    " \n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        \n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "        \n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        \n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_net(self.q_params, elem).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149f671",
   "metadata": {},
   "source": [
    "### Train Quantum CNN with angular embedding for different depth sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd00e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in init self.q_params Parameter containing:\n",
      "tensor([-0.0086, -0.0031, -0.0005, -0.0050], requires_grad=True)\n",
      "########## resnet_q_angular_depth1 ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.655\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.672\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.661\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.673\n",
      "Accuracy test: 0.565\n",
      "Time 2.69\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.766\n",
      "Accuracy train: 0.281\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.665\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.655\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.680\n",
      "Accuracy test: 0.540\n",
      "Time 2.66\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.670\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.56\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.669\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.640\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.620\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.629\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.646\n",
      "Accuracy test: 0.790\n",
      "Time 2.60\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.594\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.603\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.593\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.616\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.571\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.602\n",
      "Accuracy test: 0.875\n",
      "Time 2.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.634\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.571\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.49\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.598\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.565\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.578\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.582\n",
      "Accuracy test: 0.910\n",
      "Time 2.62\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.615\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.536\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.566\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.574\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.567\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.553\n",
      "Accuracy test: 0.940\n",
      "Time 2.72\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.536\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.549\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.537\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.523\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.537\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.529\n",
      "Accuracy test: 0.960\n",
      "Time 2.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.552\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.508\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.517\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.544\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.527\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.513\n",
      "Accuracy test: 0.980\n",
      "Time 2.66\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.510\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.531\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.544\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.509\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.537\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.499\n",
      "Accuracy test: 0.980\n",
      "Time 3.05\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.488\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.484\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.502\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.492\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.524\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.487\n",
      "Accuracy test: 0.985\n",
      "Time 2.65\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.485\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.488\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.56\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.481\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.470\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.476\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.478\n",
      "Accuracy test: 0.980\n",
      "Time 2.70\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.479\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.61\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.475\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.488\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.479\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.500\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.467\n",
      "Accuracy test: 0.985\n",
      "Time 2.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.469\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.488\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.475\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.467\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.465\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.461\n",
      "Accuracy test: 0.975\n",
      "Time 2.68\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.452\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.458\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.466\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.477\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.439\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.449\n",
      "Accuracy test: 0.985\n",
      "Time 2.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.450\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.443\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.425\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.425\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.464\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.442\n",
      "Accuracy test: 0.985\n",
      "Time 2.63\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([-0.0175, -0.0014,  0.0053,  0.0071], requires_grad=True)\n",
      "\n",
      "########## run 1 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.884\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.712\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.607\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.642\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.625\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.765\n",
      "Accuracy test: 0.435\n",
      "Time 2.72\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.665\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.633\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.622\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.635\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.580\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.625\n",
      "Accuracy test: 0.775\n",
      "Time 2.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.598\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.551\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.467\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.463\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.517\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.653\n",
      "Accuracy test: 0.600\n",
      "Time 2.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.547\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.528\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.477\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.543\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.483\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.56\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.544\n",
      "Accuracy test: 0.795\n",
      "Time 2.75\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.493\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.455\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.420\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.406\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.57\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.435\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.484\n",
      "Accuracy test: 0.835\n",
      "Time 2.65\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.372\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.335\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.464\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.373\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.320\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.440\n",
      "Accuracy test: 0.865\n",
      "Time 2.60\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.387\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.346\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.387\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.385\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.352\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.388\n",
      "Accuracy test: 0.930\n",
      "Time 2.58\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.351\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.323\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.44\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.342\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.325\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.282\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.351\n",
      "Accuracy test: 0.930\n",
      "Time 2.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.302\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.40\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.327\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.456\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.290\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.246\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.332\n",
      "Accuracy test: 0.930\n",
      "Time 2.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.330\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.296\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.325\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.307\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.312\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.300\n",
      "Accuracy test: 0.970\n",
      "Time 2.57\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.224\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.251\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.285\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.297\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.306\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.281\n",
      "Accuracy test: 0.970\n",
      "Time 2.57\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.223\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.51\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.237\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.41\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.291\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.243\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.257\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.287\n",
      "Accuracy test: 0.950\n",
      "Time 2.59\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.261\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.434\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.244\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.255\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.283\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.271\n",
      "Accuracy test: 0.960\n",
      "Time 2.60\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.311\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.237\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.254\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.235\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.255\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.255\n",
      "Accuracy test: 0.960\n",
      "Time 2.57\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.278\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.254\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.223\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.242\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.199\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.245\n",
      "Accuracy test: 0.970\n",
      "Time 2.57\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 0.0063,  0.0063, -0.0003,  0.0006], requires_grad=True)\n",
      "\n",
      "########## run 2 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.825\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 1.50\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.766\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.717\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.727\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.693\n",
      "Accuracy test: 0.565\n",
      "Time 2.72\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.725\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 1.49\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.565\n",
      "Time 2.66\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.663\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 1.53\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.666\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.661\n",
      "Accuracy test: 0.565\n",
      "Time 2.67\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.663\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.646\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.650\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.648\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.635\n",
      "Accuracy test: 0.595\n",
      "Time 2.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.635\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.596\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.636\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.555\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.585\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.574\n",
      "Accuracy test: 0.600\n",
      "Time 2.66\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.574\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.601\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.541\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.56\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.559\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.568\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.530\n",
      "Accuracy test: 0.785\n",
      "Time 2.71\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.523\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.528\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.55\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.516\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.601\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.500\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.49\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.488\n",
      "Accuracy test: 0.905\n",
      "Time 2.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.511\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.491\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.52\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.500\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.484\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.36\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.499\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.450\n",
      "Accuracy test: 0.915\n",
      "Time 2.60\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.423\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.478\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.564\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.471\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.449\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.423\n",
      "Accuracy test: 0.945\n",
      "Time 2.58\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.457\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.445\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.409\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.450\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.418\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.403\n",
      "Accuracy test: 0.945\n",
      "Time 2.58\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.523\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.473\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.389\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.387\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.384\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.383\n",
      "Accuracy test: 0.955\n",
      "Time 2.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.411\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.403\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.367\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.411\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.355\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.368\n",
      "Accuracy test: 0.965\n",
      "Time 2.57\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.357\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.385\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.389\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.352\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.399\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.359\n",
      "Accuracy test: 0.970\n",
      "Time 2.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.354\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.40\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.367\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.38\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.388\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.397\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.351\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.340\n",
      "Accuracy test: 0.975\n",
      "Time 2.59\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.358\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.47\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.350\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.45\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.329\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.420\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 1.39\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.351\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 1.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.330\n",
      "Accuracy test: 0.970\n",
      "Time 2.56\n",
      "#####################\n",
      "\n",
      "total time taken 467.87\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([-0.0095,  0.0009, -0.0066, -0.0078, -0.0077, -0.0237, -0.0004,  0.0007,\n",
      "         0.0202, -0.0191, -0.0042,  0.0104], requires_grad=True)\n",
      "########## resnet_q_angular_depth3 ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.626\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.625\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.597\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.561\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.546\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.673\n",
      "Accuracy test: 0.450\n",
      "Time 2.94\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.610\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.561\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.531\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.532\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.589\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.616\n",
      "Accuracy test: 0.770\n",
      "Time 2.94\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.524\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.87\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.565\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.508\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.503\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.578\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.577\n",
      "Accuracy test: 0.845\n",
      "Time 2.94\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.477\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.485\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.77\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.572\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.494\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.474\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.71\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.531\n",
      "Accuracy test: 0.915\n",
      "Time 3.00\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.487\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.476\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.425\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.432\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.401\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.493\n",
      "Accuracy test: 0.890\n",
      "Time 2.95\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.390\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.80\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.370\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.81\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.555\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.517\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.401\n",
      "Accuracy train: 0.875\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.472\n",
      "Accuracy test: 0.870\n",
      "Time 2.87\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.549\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.458\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.401\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.76\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.369\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.410\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.455\n",
      "Accuracy test: 0.955\n",
      "Time 2.95\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.415\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.403\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.433\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.408\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.419\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.75\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.419\n",
      "Accuracy test: 0.965\n",
      "Time 2.86\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.417\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.80\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.383\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.348\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.354\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.77\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.342\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.80\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.404\n",
      "Accuracy test: 0.930\n",
      "Time 2.95\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.336\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.385\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.334\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.76\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.451\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 4.75\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.358\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.75\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.376\n",
      "Accuracy test: 0.965\n",
      "Time 3.03\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.331\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.77\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.307\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.342\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.327\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.396\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.360\n",
      "Accuracy test: 0.985\n",
      "Time 2.97\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.356\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.368\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.345\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.74\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.329\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.350\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.80\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.354\n",
      "Accuracy test: 0.955\n",
      "Time 3.00\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.367\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.76\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.432\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.336\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.340\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.311\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.337\n",
      "Accuracy test: 0.965\n",
      "Time 2.95\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.314\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.76\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.309\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.81\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.344\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.314\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.75\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.307\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.324\n",
      "Accuracy test: 0.990\n",
      "Time 2.88\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.300\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.315\n",
      "Accuracy train: 1.000\n",
      "Loss train: 0.322\n",
      "Accuracy train: 0.969\n",
      "Time per batch_iter 4.71\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.371\n",
      "Accuracy train: 0.938\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.278\n",
      "Accuracy train: 1.000\n",
      "Time per batch_iter 4.78\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.309\n",
      "Accuracy test: 0.990\n",
      "Time 2.85\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 0.0138,  0.0166, -0.0015, -0.0074, -0.0036,  0.0205, -0.0039,  0.0103,\n",
      "        -0.0153, -0.0128,  0.0082, -0.0018], requires_grad=True)\n",
      "\n",
      "########## run 1 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.734\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.655\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 4.72\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 4.79\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.642\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.613\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 4.73\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.697\n",
      "Accuracy test: 0.495\n",
      "Time 3.00\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.549\n",
      "Accuracy train: 0.812\n"
     ]
    }
   ],
   "source": [
    "num_runs = 3\n",
    "num_epochs = 3\n",
    "n_qubits = 4\n",
    "\n",
    "for q_depth in [1, 3]:\n",
    "\n",
    "    q_avg_acc_train, q_avg_acc_test, q_avg_loss_train, q_avg_loss_test = run_experiment(num_runs, num_epochs, nn_type = 'angular', q_depth = q_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf66cc8",
   "metadata": {},
   "source": [
    "### Plot angular embedding metric development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88694bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for q_depth in [1, 3]:\n",
    "    \n",
    "    model_name=f\"resnet_q_angular_depth{q_depth}\"\n",
    "    \n",
    "    # load quantum metrics\n",
    "    q_avg_acc_test = np.load(f'./metrics/{model_name}/avg_acc_test.npy')\n",
    "    q_avg_acc_train = np.load(f'./metrics/{model_name}/avg_acc_train.npy')\n",
    "\n",
    "    q_avg_loss_train = np.load(f'./metrics/{model_name}/avg_loss_train.npy')\n",
    "    q_avg_loss_test = np.load(f'./metrics/{model_name}/avg_loss_test.npy')\n",
    "    \n",
    "    print(f'#### DEPTH {q_depth} ####')\n",
    "    \n",
    "    print(f'max average test accuracy {max(q_avg_acc_test.mean(axis=0)):.3f}')\n",
    "\n",
    "    plot_stuff(q_avg_acc_test, title = 'Acc test',plot_each_metric=False)\n",
    "    plot_stuff(q_avg_loss_test, title = 'Loss test',plot_each_metric=False)\n",
    "    \n",
    "    plot_stuff(q_avg_acc_train, title = 'Acc train',plot_each_metric=False)\n",
    "    plot_stuff(q_avg_loss_train, title = 'Loss train',plot_each_metric=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca657ac",
   "metadata": {},
   "source": [
    "### Plot angular embedding vs classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b05c2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff_combined(data_list, labels_list, title):\n",
    "    \n",
    "    plt.figure(figsize=(12,7))\n",
    "    for idx, data_ in enumerate(data_list):\n",
    "\n",
    "        data = data_.mean(axis=0)\n",
    "        std = data_.std(axis=0)\n",
    "\n",
    "        plt.title(title)\n",
    "\n",
    "        plt.plot(data, label=labels_list[idx])\n",
    "\n",
    "        plt.fill_between(range(len(data)), data - std, data + std, alpha=0.15)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# load classic metrics \n",
    "avg_acc_test = np.load('./metrics/resnet/avg_acc_test.npy')\n",
    "avg_acc_train = np.load('./metrics/resnet/avg_acc_train.npy')\n",
    "\n",
    "avg_loss_train = np.load('./metrics/resnet/avg_loss_train.npy')\n",
    "avg_loss_test = np.load('./metrics/resnet/avg_loss_test.npy')\n",
    "\n",
    "acc_test_list = []\n",
    "acc_train_list = []\n",
    "\n",
    "loss_test_list = []\n",
    "loss_train_list = []\n",
    "\n",
    "labels_list = []\n",
    "\n",
    "acc_test_list.append(avg_acc_test)\n",
    "acc_train_list.append(avg_acc_train)\n",
    "\n",
    "loss_test_list.append(avg_loss_test)\n",
    "loss_train_list.append(avg_loss_train)\n",
    "\n",
    "labels_list.append(\"Classic\")\n",
    "\n",
    "\n",
    "for q_depth in [1, 3]:\n",
    "    \n",
    "    model_name=f\"resnet_q_angular_depth{q_depth}\"\n",
    "    \n",
    "    print(f'########## {model_name} ##########')\n",
    "\n",
    "    # load quantum metrics\n",
    "    avg_acc_test = np.load(f'./metrics/{model_name}/avg_acc_test.npy')\n",
    "    avg_acc_train = np.load(f'./metrics/{model_name}/avg_acc_train.npy')\n",
    "\n",
    "    avg_loss_train = np.load(f'./metrics/{model_name}/avg_loss_train.npy')\n",
    "    avg_loss_test = np.load(f'./metrics/{model_name}/avg_loss_test.npy')\n",
    "    \n",
    "    acc_test_list.append(avg_acc_test)\n",
    "    acc_train_list.append(avg_acc_train)\n",
    "\n",
    "    loss_test_list.append(avg_loss_test)\n",
    "    loss_train_list.append(avg_loss_train)\n",
    "    \n",
    "    labels_list.append(f\"Q_Resnet_depth_{q_depth}\")\n",
    "\n",
    "# plot in one plot\n",
    "plot_stuff_combined(acc_test_list, labels_list, title = 'Acc test')\n",
    "plot_stuff_combined(loss_test_list, labels_list, title = 'Loss test')\n",
    "\n",
    "plot_stuff_combined(acc_train_list, labels_list, title = 'Acc train')\n",
    "plot_stuff_combined(loss_train_list, labels_list, title = 'Loss train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc294c",
   "metadata": {},
   "source": [
    "### set up quantum net with amplitude embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55736929",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_weights_flat tensor([-0.2306, -0.8960,  0.4267, -0.6918, -1.0785,  0.0098,  0.5169,  0.6629,\n",
      "        -0.7725,  0.1998,  0.5384, -0.4536, -2.2713, -0.9788, -1.5580,  0.2572,\n",
      "        -1.0505,  0.5416])\n",
      "quantum_net_amplitude.draw()\n",
      " 0: QubitStateVector(M0)RY(-0.231)RX(-0.896)CX     \n",
      " 1: QubitStateVector(M0)RY(0.427)RX(-0.692)XC     \n",
      " 2: QubitStateVector(M0)RY(-1.08)RX(0.00979)XC     \n",
      " 3: QubitStateVector(M0)RY(0.517)RX(0.663)XC     \n",
      " 4: QubitStateVector(M0)RY(-0.773)RX(0.2)XC     \n",
      " 5: QubitStateVector(M0)RY(0.538)RX(-0.454)XC     \n",
      " 6: QubitStateVector(M0)RY(-2.27)RX(-0.979)XC     \n",
      " 7: QubitStateVector(M0)RY(-1.56)RX(0.257)XC Z \n",
      " 8: QubitStateVector(M0)RY(-1.05)RX(0.542)XC Z \n",
      "M0 =\n",
      "tensor([-0.0524+0.j,  0.0329+0.j, -0.0506+0.j, -0.0443+0.j, -0.0411+0.j,  0.0108+0.j,\n",
      "        -0.0950+0.j,  0.0331+0.j, -0.0028+0.j,  0.0321+0.j,  0.0531+0.j,  0.0344+0.j,\n",
      "         0.0034+0.j,  0.0657+0.j, -0.0981+0.j,  0.0667+0.j, -0.0261+0.j,  0.0374+0.j,\n",
      "         0.0821+0.j, -0.0439+0.j, -0.0851+0.j, -0.0080+0.j, -0.0071+0.j,  0.0742+0.j,\n",
      "        -0.0424+0.j,  0.0150+0.j,  0.0102+0.j, -0.0246+0.j,  0.0180+0.j, -0.0119+0.j,\n",
      "         0.0417+0.j,  0.0467+0.j,  0.0078+0.j,  0.0064+0.j, -0.0328+0.j, -0.0504+0.j,\n",
      "        -0.0467+0.j, -0.0058+0.j, -0.0045+0.j, -0.0584+0.j, -0.0205+0.j,  0.0518+0.j,\n",
      "        -0.0034+0.j,  0.0555+0.j, -0.0227+0.j,  0.0064+0.j, -0.0334+0.j,  0.0063+0.j,\n",
      "        -0.0332+0.j, -0.0787+0.j,  0.0027+0.j,  0.1300+0.j, -0.0611+0.j,  0.0350+0.j,\n",
      "         0.0431+0.j,  0.0646+0.j,  0.0662+0.j,  0.0307+0.j,  0.0379+0.j, -0.0510+0.j,\n",
      "        -0.0115+0.j, -0.0774+0.j,  0.0663+0.j, -0.0845+0.j,  0.0027+0.j,  0.0035+0.j,\n",
      "         0.0160+0.j,  0.0439+0.j,  0.0822+0.j, -0.0374+0.j,  0.0385+0.j,  0.0308+0.j,\n",
      "        -0.0620+0.j, -0.0484+0.j,  0.0387+0.j, -0.0047+0.j,  0.0540+0.j, -0.0020+0.j,\n",
      "         0.0754+0.j,  0.0117+0.j, -0.0492+0.j, -0.0743+0.j, -0.0185+0.j, -0.0378+0.j,\n",
      "         0.0056+0.j, -0.0041+0.j,  0.0162+0.j,  0.0434+0.j,  0.0218+0.j, -0.0267+0.j,\n",
      "         0.0006+0.j, -0.0548+0.j, -0.0706+0.j, -0.0393+0.j,  0.0511+0.j, -0.0192+0.j,\n",
      "         0.0486+0.j,  0.0522+0.j,  0.1003+0.j, -0.0365+0.j,  0.0253+0.j,  0.0076+0.j,\n",
      "        -0.0315+0.j, -0.0021+0.j,  0.0491+0.j, -0.0619+0.j, -0.0274+0.j,  0.0399+0.j,\n",
      "        -0.0243+0.j, -0.0504+0.j,  0.0486+0.j,  0.0473+0.j, -0.0288+0.j,  0.0289+0.j,\n",
      "        -0.0054+0.j, -0.0402+0.j,  0.0335+0.j,  0.0506+0.j, -0.0490+0.j,  0.0458+0.j,\n",
      "        -0.0159+0.j, -0.0354+0.j,  0.0438+0.j, -0.0196+0.j, -0.0331+0.j, -0.0406+0.j,\n",
      "         0.0098+0.j,  0.0038+0.j,  0.0383+0.j,  0.0464+0.j,  0.0197+0.j,  0.0637+0.j,\n",
      "         0.0006+0.j, -0.0041+0.j, -0.0022+0.j, -0.0040+0.j, -0.0971+0.j, -0.0171+0.j,\n",
      "         0.0095+0.j, -0.0206+0.j, -0.0384+0.j,  0.0118+0.j, -0.0430+0.j, -0.0230+0.j,\n",
      "         0.0146+0.j,  0.0068+0.j, -0.0219+0.j,  0.0032+0.j, -0.0006+0.j, -0.0073+0.j,\n",
      "         0.0707+0.j,  0.0762+0.j,  0.0210+0.j, -0.0144+0.j, -0.0424+0.j,  0.1162+0.j,\n",
      "        -0.0152+0.j, -0.0168+0.j, -0.0147+0.j, -0.0621+0.j,  0.0496+0.j,  0.0782+0.j,\n",
      "        -0.0750+0.j, -0.0435+0.j, -0.0147+0.j,  0.0840+0.j,  0.0490+0.j, -0.0470+0.j,\n",
      "        -0.0411+0.j,  0.0620+0.j,  0.0107+0.j, -0.0211+0.j,  0.0245+0.j,  0.0331+0.j,\n",
      "        -0.0066+0.j,  0.0109+0.j,  0.0185+0.j,  0.0021+0.j,  0.0395+0.j,  0.0189+0.j,\n",
      "        -0.0966+0.j, -0.0602+0.j,  0.0168+0.j, -0.0542+0.j, -0.0165+0.j,  0.0821+0.j,\n",
      "         0.0933+0.j, -0.0245+0.j, -0.0240+0.j,  0.0408+0.j, -0.0478+0.j, -0.0690+0.j,\n",
      "         0.0391+0.j, -0.0228+0.j,  0.0121+0.j,  0.0237+0.j,  0.0183+0.j,  0.0479+0.j,\n",
      "        -0.0231+0.j,  0.0282+0.j, -0.0643+0.j, -0.0208+0.j,  0.0518+0.j, -0.0157+0.j,\n",
      "        -0.0324+0.j,  0.0500+0.j, -0.0494+0.j, -0.0590+0.j,  0.0063+0.j,  0.0106+0.j,\n",
      "         0.0155+0.j, -0.0463+0.j,  0.0248+0.j,  0.0391+0.j,  0.0943+0.j,  0.0515+0.j,\n",
      "        -0.0096+0.j,  0.0153+0.j, -0.0030+0.j, -0.0035+0.j,  0.0712+0.j, -0.0046+0.j,\n",
      "        -0.0159+0.j, -0.0105+0.j, -0.0714+0.j, -0.0601+0.j,  0.0074+0.j,  0.0014+0.j,\n",
      "         0.0080+0.j, -0.0446+0.j,  0.0164+0.j,  0.0270+0.j,  0.0087+0.j, -0.0100+0.j,\n",
      "        -0.0455+0.j, -0.0335+0.j,  0.0575+0.j, -0.0449+0.j,  0.0251+0.j,  0.0102+0.j,\n",
      "         0.0092+0.j,  0.0076+0.j,  0.0379+0.j, -0.0197+0.j,  0.0730+0.j,  0.0776+0.j,\n",
      "         0.0571+0.j, -0.0799+0.j, -0.0370+0.j,  0.0197+0.j,  0.0504+0.j,  0.0188+0.j,\n",
      "        -0.0081+0.j,  0.0070+0.j,  0.0398+0.j,  0.0958+0.j,  0.0408+0.j, -0.1030+0.j,\n",
      "        -0.0338+0.j, -0.0266+0.j,  0.0128+0.j, -0.0337+0.j,  0.0110+0.j,  0.0406+0.j,\n",
      "        -0.0617+0.j,  0.0030+0.j, -0.0141+0.j, -0.0503+0.j, -0.0584+0.j,  0.0935+0.j,\n",
      "         0.0412+0.j, -0.0402+0.j,  0.0296+0.j, -0.0514+0.j, -0.0650+0.j,  0.0560+0.j,\n",
      "         0.0549+0.j,  0.0317+0.j,  0.0524+0.j, -0.0075+0.j, -0.0469+0.j, -0.0722+0.j,\n",
      "         0.0172+0.j, -0.0252+0.j,  0.0198+0.j, -0.0674+0.j,  0.0080+0.j,  0.0314+0.j,\n",
      "        -0.0398+0.j, -0.0471+0.j,  0.0519+0.j, -0.0180+0.j,  0.0591+0.j, -0.0045+0.j,\n",
      "         0.0718+0.j,  0.0539+0.j,  0.0096+0.j,  0.0087+0.j, -0.0465+0.j, -0.0183+0.j,\n",
      "        -0.0530+0.j, -0.0107+0.j, -0.0407+0.j, -0.0296+0.j, -0.0041+0.j,  0.0144+0.j,\n",
      "         0.0769+0.j,  0.0136+0.j, -0.0218+0.j,  0.0365+0.j,  0.0468+0.j,  0.0420+0.j,\n",
      "         0.0401+0.j, -0.0590+0.j, -0.0258+0.j, -0.0032+0.j,  0.0268+0.j, -0.0132+0.j,\n",
      "        -0.0644+0.j, -0.0154+0.j,  0.0070+0.j,  0.0591+0.j, -0.0248+0.j,  0.0188+0.j,\n",
      "         0.0033+0.j, -0.0455+0.j,  0.0933+0.j, -0.0476+0.j, -0.0192+0.j, -0.0063+0.j,\n",
      "         0.0025+0.j,  0.0048+0.j, -0.0006+0.j, -0.0143+0.j,  0.0117+0.j,  0.0056+0.j,\n",
      "         0.0329+0.j,  0.0033+0.j, -0.0243+0.j,  0.0436+0.j,  0.0219+0.j,  0.0402+0.j,\n",
      "         0.0169+0.j, -0.0286+0.j, -0.0110+0.j, -0.0140+0.j,  0.0030+0.j, -0.0362+0.j,\n",
      "         0.0585+0.j,  0.0565+0.j, -0.0397+0.j,  0.0419+0.j, -0.0595+0.j, -0.0098+0.j,\n",
      "        -0.0825+0.j, -0.0988+0.j, -0.0221+0.j,  0.0046+0.j, -0.0380+0.j,  0.0567+0.j,\n",
      "        -0.0697+0.j, -0.0456+0.j, -0.0522+0.j, -0.0824+0.j, -0.0401+0.j, -0.0622+0.j,\n",
      "         0.0754+0.j,  0.0341+0.j, -0.0152+0.j,  0.0443+0.j, -0.0406+0.j, -0.0112+0.j,\n",
      "         0.0105+0.j,  0.0366+0.j,  0.0213+0.j,  0.0023+0.j, -0.0248+0.j, -0.0218+0.j,\n",
      "         0.0659+0.j,  0.0765+0.j, -0.0545+0.j,  0.0569+0.j, -0.0599+0.j,  0.0388+0.j,\n",
      "        -0.0491+0.j,  0.0070+0.j,  0.0057+0.j,  0.0011+0.j,  0.0336+0.j,  0.0078+0.j,\n",
      "        -0.0315+0.j,  0.0017+0.j, -0.0246+0.j, -0.0658+0.j, -0.0444+0.j,  0.0103+0.j,\n",
      "        -0.0084+0.j, -0.0318+0.j,  0.0605+0.j,  0.0250+0.j, -0.0158+0.j,  0.0282+0.j,\n",
      "        -0.0448+0.j, -0.0328+0.j, -0.0071+0.j,  0.0622+0.j,  0.0018+0.j, -0.0420+0.j,\n",
      "        -0.0388+0.j, -0.0430+0.j,  0.0378+0.j,  0.0027+0.j, -0.0275+0.j,  0.0736+0.j,\n",
      "         0.0081+0.j, -0.0183+0.j, -0.0411+0.j, -0.0322+0.j,  0.0489+0.j, -0.0543+0.j,\n",
      "        -0.0427+0.j, -0.0560+0.j, -0.0219+0.j,  0.0021+0.j,  0.0482+0.j,  0.0803+0.j,\n",
      "         0.0087+0.j,  0.0543+0.j,  0.0546+0.j, -0.0363+0.j, -0.0851+0.j, -0.0606+0.j,\n",
      "         0.1010+0.j, -0.0013+0.j, -0.0288+0.j, -0.0348+0.j, -0.0251+0.j, -0.0243+0.j,\n",
      "        -0.0389+0.j, -0.0256+0.j,  0.0742+0.j,  0.0331+0.j, -0.0163+0.j,  0.0149+0.j,\n",
      "         0.0020+0.j,  0.0172+0.j,  0.0138+0.j, -0.1031+0.j,  0.0864+0.j, -0.0731+0.j,\n",
      "         0.0306+0.j,  0.0404+0.j, -0.0092+0.j, -0.0500+0.j, -0.0294+0.j, -0.0046+0.j,\n",
      "        -0.0632+0.j, -0.1053+0.j,  0.0325+0.j,  0.0238+0.j,  0.0171+0.j,  0.0430+0.j,\n",
      "        -0.0307+0.j,  0.0274+0.j,  0.0098+0.j, -0.0302+0.j, -0.0518+0.j, -0.0418+0.j,\n",
      "        -0.0026+0.j, -0.0444+0.j, -0.0939+0.j,  0.0149+0.j, -0.0052+0.j, -0.0169+0.j,\n",
      "        -0.0042+0.j,  0.0280+0.j,  0.0603+0.j, -0.0280+0.j,  0.0306+0.j, -0.0197+0.j,\n",
      "         0.0718+0.j,  0.0522+0.j, -0.0400+0.j, -0.0497+0.j,  0.0074+0.j,  0.0343+0.j,\n",
      "         0.0212+0.j,  0.0885+0.j,  0.0559+0.j,  0.0572+0.j, -0.0824+0.j,  0.0391+0.j,\n",
      "         0.0798+0.j,  0.0157+0.j, -0.0584+0.j, -0.0542+0.j, -0.0083+0.j, -0.0238+0.j,\n",
      "         0.0746+0.j, -0.0115+0.j,  0.0457+0.j, -0.0177+0.j, -0.0783+0.j, -0.0366+0.j,\n",
      "         0.0082+0.j,  0.0120+0.j,  0.0295+0.j, -0.0393+0.j, -0.0465+0.j, -0.0697+0.j,\n",
      "         0.0341+0.j,  0.0841+0.j], dtype=torch.complex128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# amplitude embedding \n",
    "n_qubits = 9\n",
    "q_depth = 1\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net_amplitude(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits, 2)\n",
    "\n",
    "    amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        \n",
    "        RY_layer(q_weights[k,:, 0])\n",
    "        RX_layer(q_weights[k,:, 1])       \n",
    "        \n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "    \n",
    "    # measure last two qubitss\n",
    "    return tuple([qml.expval(qml.PauliZ(qubit)) for qubit in range(n_qubits - 2, n_qubits)])\n",
    "\n",
    "\n",
    "q_input_features = torch.randn(2 ** n_qubits,requires_grad=False)\n",
    "q_weights_flat = torch.randn(q_depth * n_qubits * 2) \n",
    "\n",
    "print('q_weights_flat',q_weights_flat)\n",
    "result = quantum_net_amplitude(q_weights_flat, q_input_features)\n",
    "\n",
    "print('quantum_net_amplitude.draw()')\n",
    "print(quantum_net_amplitude.draw())\n",
    "\n",
    "class QuantumNetAmplitudeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits, q_depth):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q_params = nn.Parameter( np.pi * ( torch.rand(q_depth * n_qubits * 2) * 2 - 1) )\n",
    "        print('in init self.q_params', self.q_params)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, 2)\n",
    "        q_out = q_out.to(device)\n",
    "        \n",
    "        # iterate over each element of the batch\n",
    "        for elem in input_features:\n",
    "            q_out_elem = quantum_net_amplitude(self.q_params, elem).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "        \n",
    "        # transform q_out \\in [-1,1] to |R\n",
    "        q_out = torch.arctanh(q_out)\n",
    "        \n",
    "        return q_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2192e",
   "metadata": {},
   "source": [
    "### Train QNN with amplitude embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918e5c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in init self.q_params Parameter containing:\n",
      "tensor([ 2.6829,  0.8089,  3.1119,  1.5353, -2.4908, -1.3787,  0.6003, -1.4313,\n",
      "        -1.7272, -1.7275,  2.6575, -0.0796,  1.7549, -3.0907,  0.9303, -1.6096,\n",
      "         1.2881, -2.3100], requires_grad=True)\n",
      "########## resnet_q_amplitude_depth1 ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.55\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.440\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.55\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.475\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.696\n",
      "Accuracy test: 0.440\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.51\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.696\n",
      "Accuracy test: 0.405\n",
      "Time 2.80\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.435\n",
      "Time 2.80\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.480\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.64\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.68\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.693\n",
      "Accuracy test: 0.485\n",
      "Time 2.86\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.69\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.62\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.62\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.312\n",
      "Time per batch_iter 2.70\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.64\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.692\n",
      "Accuracy test: 0.520\n",
      "Time 2.87\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.70\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.74\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.62\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.69\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.692\n",
      "Accuracy test: 0.540\n",
      "Time 2.88\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.68\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.69\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.555\n",
      "Time 2.88\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.68\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.62\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.70\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.575\n",
      "Time 2.85\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.64\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.70\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.63\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.595\n",
      "Time 2.92\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.69\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.55\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.600\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.625\n",
      "Time 2.83\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.600\n",
      "Time 2.80\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([-1.6796e-02,  2.6484e+00, -6.2988e-01,  2.0853e+00,  2.9918e+00,\n",
      "         5.4180e-01,  3.2692e-01, -2.4345e+00,  7.0327e-01,  1.7384e+00,\n",
      "         1.5948e+00,  2.5700e+00, -1.6078e+00,  1.2384e+00, -2.2806e+00,\n",
      "        -2.8680e-03,  1.7579e+00,  5.3714e-01], requires_grad=True)\n",
      "\n",
      "########## run 1 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.709\n",
      "Accuracy train: 0.250\n",
      "Time per batch_iter 2.45\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.705\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.312\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.692\n",
      "Accuracy test: 0.485\n",
      "Time 2.77\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.697\n",
      "Accuracy test: 0.445\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.281\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.698\n",
      "Accuracy test: 0.430\n",
      "Time 2.77\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.705\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.43\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.697\n",
      "Accuracy test: 0.415\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.65\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.440\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.51\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.485\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.705\n",
      "Accuracy train: 0.281\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.480\n",
      "Time 2.76\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.702\n",
      "Accuracy train: 0.312\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.500\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.704\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.470\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.50\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.55\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.505\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.500\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.505\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.485\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.44\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.490\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.51\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.490\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 2.8901,  1.0615,  3.1106, -0.6263, -1.4823, -1.4462, -0.1058,  1.4048,\n",
      "        -2.7396,  2.6712,  2.1477, -0.2746, -3.0398,  1.0706,  1.3432,  2.8887,\n",
      "         0.8830,  0.3854], requires_grad=True)\n",
      "\n",
      "########## run 2 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.665\n",
      "Time 2.79\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.66\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.680\n",
      "Accuracy test: 0.650\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.56\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.645\n",
      "Time 2.83\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.650\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.625\n",
      "Time 2.82\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.630\n",
      "Time 2.78\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.655\n",
      "Time 2.87\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.640\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.55\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.660\n",
      "Time 2.77\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.49\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.670\n",
      "Time 2.81\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.680\n",
      "Accuracy test: 0.665\n",
      "Time 2.80\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.680\n",
      "Accuracy test: 0.665\n",
      "Time 2.82\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.46\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.655\n",
      "Time 3.07\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 2.54\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 2.48\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 2.50\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.53\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.675\n",
      "Time 2.80\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.47\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 2.52\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.675\n",
      "Time 2.82\n",
      "#####################\n",
      "\n",
      "total time taken 703.91\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 1.9595,  2.0656, -2.9935,  1.1421, -1.9542, -0.3893, -1.9979, -2.6742,\n",
      "        -1.0896,  0.9393, -2.2401,  1.3795,  1.7679, -1.5955,  1.1794,  2.3922,\n",
      "        -0.3213,  1.4855, -0.8956,  1.2544,  2.8043, -1.2397, -0.8637, -1.8950,\n",
      "         2.2371, -1.1254,  1.0071,  2.3447, -1.7651, -0.1065, -1.8137, -0.1927,\n",
      "        -3.0950,  2.4941,  2.5418, -1.5587,  0.8714,  0.5358,  2.2447, -1.0315,\n",
      "        -1.6121,  0.5293,  2.5809,  0.6090, -1.5221, -1.5749,  0.7075, -0.6020,\n",
      "         2.1073, -0.8391,  1.5175,  0.9596, -1.0176, -1.1031],\n",
      "       requires_grad=True)\n",
      "########## resnet_q_amplitude_depth3 ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.702\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.14\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.550\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.14\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.595\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.12\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.635\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.16\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.625\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.39\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.687\n",
      "Accuracy test: 0.645\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.60\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.625\n",
      "Time 3.85\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 16.34\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.560\n",
      "Time 3.60\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.560\n",
      "Time 3.67\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.36\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.565\n",
      "Time 3.65\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.41\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.565\n",
      "Time 3.67\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.36\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.570\n",
      "Time 3.67\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.33\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.570\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.35\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.580\n",
      "Time 3.68\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.34\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.565\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.33\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.37\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.590\n",
      "Time 3.67\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 0.4988,  2.7517,  1.1295, -2.8449, -1.8907,  1.6111,  1.9139, -3.0775,\n",
      "        -0.7261,  0.9256, -1.8723, -2.8789, -0.7898,  0.5881,  0.9814,  0.2185,\n",
      "        -2.6891, -2.8984,  0.4200,  2.2376, -2.1835, -0.5393, -0.3947, -1.5732,\n",
      "         2.5461,  2.0685,  1.5260,  0.9401,  1.9376,  2.1773,  2.8895, -1.5284,\n",
      "        -1.7606,  1.8704, -0.2015, -0.9207, -2.7849, -2.4037,  1.6249, -1.7707,\n",
      "         0.8089,  1.8365, -2.3158,  1.6028,  2.2052,  3.0187,  2.1534, -2.1155,\n",
      "         2.0953, -1.7455, -0.7569,  2.6511,  1.2111,  0.5208],\n",
      "       requires_grad=True)\n",
      "\n",
      "########## run 1 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.670\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.765\n",
      "Time 3.67\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.17\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.765\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.760\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.715\n",
      "Time 3.64\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.16\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.765\n",
      "Time 3.65\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.745\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.33\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.85\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.730\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.672\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.740\n",
      "Time 3.65\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.34\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.735\n",
      "Time 3.84\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.14\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.730\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.14\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.17\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.735\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.48\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.755\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.671\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 16.35\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 16.16\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.677\n",
      "Accuracy test: 0.755\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.672\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.677\n",
      "Accuracy test: 0.760\n",
      "Time 3.66\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 2.9340,  0.8726, -0.7445, -1.5732,  2.2029, -1.3603, -0.1007,  2.0690,\n",
      "         1.2232,  1.8680,  2.7830,  0.3001,  2.6008,  2.2614,  0.0148, -2.4083,\n",
      "         0.4448, -1.2269, -3.0140,  1.8013,  1.7093, -2.4148,  2.6294, -2.0370,\n",
      "         0.6422,  0.9673, -0.0449,  0.6203,  2.1589, -3.1332, -2.0581, -2.7041,\n",
      "        -2.3140, -2.0641, -1.4073, -3.0592, -0.7825,  2.1529, -2.5692, -0.3780,\n",
      "        -0.3930, -0.0357, -1.3283,  2.0011,  0.0758, -2.9382,  2.5147,  0.8995,\n",
      "        -1.7397,  1.3259,  0.1716, -3.0146, -1.7410, -2.6655],\n",
      "       requires_grad=True)\n",
      "\n",
      "########## run 2 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.312\n",
      "Time per batch_iter 16.41\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.704\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.500\n",
      "Time 3.65\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.30\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.688\n",
      "Accuracy test: 0.510\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.701\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.688\n",
      "Accuracy test: 0.525\n",
      "Time 3.64\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.505\n",
      "Time 3.41\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.490\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.500\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.706\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.16\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.690\n",
      "Accuracy test: 0.510\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.31\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 16.28\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.530\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.17\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.689\n",
      "Accuracy test: 0.545\n",
      "Time 3.61\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.17\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.34\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.687\n",
      "Accuracy test: 0.555\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.19\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.32\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.686\n",
      "Accuracy test: 0.560\n",
      "Time 3.62\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.46\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.20\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 16.23\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.21\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.550\n",
      "Time 3.65\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.18\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 16.29\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.281\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.560\n",
      "Time 3.63\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.27\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.709\n",
      "Accuracy train: 0.312\n",
      "Time per batch_iter 16.16\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 16.26\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 16.24\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.15\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.565\n",
      "Time 3.72\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 16.41\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 16.17\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 16.34\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 16.22\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 16.25\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.580\n",
      "Time 3.64\n",
      "#####################\n",
      "\n",
      "total time taken 3833.17\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([-1.6738,  1.9364,  1.4335,  2.7753,  2.9816,  2.9913,  0.5601, -1.8705,\n",
      "        -1.4810, -0.4334,  1.7169,  3.0510, -2.6236, -2.2235, -2.2812, -2.3510,\n",
      "         1.7184,  2.7956,  2.8677, -2.9373, -0.1040,  2.2730,  1.1739, -0.6967,\n",
      "         1.9932, -0.0866, -1.6308, -0.9863,  2.8905,  2.2320,  0.1136,  0.6999,\n",
      "        -2.8833,  2.7593, -1.2129,  1.5863,  2.8440, -1.2891, -1.2935, -1.4967,\n",
      "         1.0751, -1.1496, -0.5286, -1.6051,  1.1764, -2.8500, -1.5645,  1.1871,\n",
      "        -1.4725, -1.8266,  0.5394,  2.1913,  3.1160,  0.7238,  1.0234,  1.3903,\n",
      "        -1.2971,  0.7197,  2.3717,  2.4952,  2.1088,  1.3612, -0.9741, -1.4157,\n",
      "        -3.0647,  0.6126, -1.4339, -1.0449,  1.8783,  0.2296,  0.2811,  2.9234,\n",
      "         1.6603,  0.8030, -2.8392,  1.5487, -2.9742, -3.0942, -2.2052,  1.9890,\n",
      "         2.3338,  0.8645, -1.8219,  2.5331,  2.5506, -2.1836, -1.6729, -1.6476,\n",
      "        -0.7366,  1.5785], requires_grad=True)\n",
      "########## resnet_q_amplitude_depth5 ##########\n",
      "\n",
      "########## run 0 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.92\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.83\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.77\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.75\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.699\n",
      "Accuracy test: 0.470\n",
      "Time 4.40\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.88\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.84\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.75\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.80\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.495\n",
      "Time 4.42\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.97\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.92\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.87\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.84\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.655\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.83\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.80\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.88\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.710\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.85\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 43.07\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.725\n",
      "Time 4.40\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.92\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.64\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.78\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.67\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.705\n",
      "Time 4.37\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.79\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.83\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.77\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.730\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.84\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.58\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.750\n",
      "Time 4.34\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.76\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.54\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.78\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.60\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.680\n",
      "Accuracy test: 0.770\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 42.58\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.57\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.765\n",
      "Time 4.40\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.80\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.70\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.79\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.785\n",
      "Time 4.37\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.70\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.70\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.66\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.775\n",
      "Time 4.39\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.77\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.61\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.672\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.677\n",
      "Accuracy test: 0.775\n",
      "Time 4.37\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.69\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.80\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.53\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.60\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.676\n",
      "Accuracy test: 0.785\n",
      "Time 4.39\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.75\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.65\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.676\n",
      "Accuracy test: 0.785\n",
      "Time 4.39\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 1.1164,  1.8819,  0.0637,  0.2237, -2.7516,  1.2425,  1.5173,  0.6027,\n",
      "         2.1918,  1.2401, -0.9458,  2.9461,  2.0849, -0.5674, -2.7593, -1.3942,\n",
      "         2.8977, -2.3349, -0.8033, -0.3861,  2.6955, -1.5043, -2.8741,  1.6011,\n",
      "        -1.0605, -0.1029, -2.2948,  0.2173, -2.3794, -1.4159,  1.9749,  0.1829,\n",
      "         1.8517, -0.1443,  0.7971,  1.1131,  1.0615, -2.3818, -2.3634, -0.1480,\n",
      "         0.1209, -0.9312, -1.6898,  2.6271,  0.4821, -2.6110, -1.2223,  0.2043,\n",
      "        -2.9777, -0.1750,  0.7947,  2.1325,  1.9443, -2.5906, -2.2063, -0.7390,\n",
      "         0.9530, -2.8556,  3.0105,  2.8792, -1.8790,  1.2435, -0.0513,  1.8375,\n",
      "         1.3571,  1.0921,  0.8614, -1.5581, -2.9076,  0.0153, -1.2605,  2.5393,\n",
      "         0.7177,  0.7954, -0.5092, -2.0342,  2.3722,  0.8517,  2.3787,  1.7156,\n",
      "        -0.1662,  1.7316, -1.1645, -1.8761,  3.0205, -1.0327,  2.5003, -1.1329,\n",
      "        -2.5106,  2.2507], requires_grad=True)\n",
      "\n",
      "########## run 1 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 45.96\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 45.79\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.90\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.70\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.87\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.695\n",
      "Accuracy test: 0.460\n",
      "Time 4.55\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 46.00\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.90\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.63\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.63\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.694\n",
      "Accuracy test: 0.485\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 46.15\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.87\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.692\n",
      "Accuracy test: 0.470\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 46.02\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 45.91\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 46.60\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.99\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.86\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.691\n",
      "Accuracy test: 0.510\n",
      "Time 4.55\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 46.20\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.79\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 45.83\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.688\n",
      "Accuracy test: 0.600\n",
      "Time 4.63\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 46.00\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 45.91\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.94\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 45.88\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.687\n",
      "Accuracy test: 0.640\n",
      "Time 4.58\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.90\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 45.89\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 45.84\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.99\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 45.84\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.630\n",
      "Time 4.57\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.84\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 46.27\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.81\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 46.03\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.86\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.670\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.99\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 46.17\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 46.18\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.90\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.681\n",
      "Accuracy test: 0.680\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.906\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 45.87\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 45.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.679\n",
      "Accuracy test: 0.705\n",
      "Time 4.54\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 46.08\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 46.03\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.84\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 45.89\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 45.90\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.678\n",
      "Accuracy test: 0.740\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.98\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.98\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 46.05\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.677\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 45.93\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.677\n",
      "Accuracy test: 0.735\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 46.02\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 45.85\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 46.04\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.673\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.83\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.676\n",
      "Accuracy test: 0.745\n",
      "Time 4.54\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.99\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 45.95\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 45.94\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.672\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.96\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.679\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 45.77\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.675\n",
      "Accuracy test: 0.765\n",
      "Time 4.56\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 46.04\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 45.91\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.812\n",
      "Time per batch_iter 45.97\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.676\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 45.89\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.674\n",
      "Accuracy test: 0.775\n",
      "Time 4.59\n",
      "#####################\n",
      "\n",
      "in init self.q_params Parameter containing:\n",
      "tensor([ 1.2920,  0.3233, -1.4807, -1.1247, -0.7931,  1.8312, -0.6934,  3.1153,\n",
      "        -3.1017, -3.0275,  2.8585,  1.0028, -1.9463, -1.3690,  1.3970,  1.2522,\n",
      "         2.3455, -2.3620,  1.5175, -2.0826,  2.1091,  2.4721, -1.2217, -1.4029,\n",
      "        -1.7175, -0.5594,  1.5217,  2.8303, -1.2885,  0.1368, -0.3614, -1.5232,\n",
      "        -1.3098, -1.1091, -2.6297,  0.1689,  2.4859,  0.0357, -1.5066,  2.1963,\n",
      "         1.3695,  2.9644,  0.9465, -2.6471,  0.4861,  1.4229, -0.7016,  2.9364,\n",
      "         0.9290,  2.9975,  0.2968,  1.0899,  2.2843, -1.0126, -1.9091,  0.5808,\n",
      "        -2.5025,  1.1026, -0.0586, -2.2008, -0.3747, -2.3894, -0.6371, -2.9277,\n",
      "        -1.3137, -3.0286, -2.7359, -1.6290,  1.9833, -2.2959, -1.3595,  2.3658,\n",
      "        -3.0690,  2.7800,  1.6199, -1.7567, -1.8598,  1.5783,  2.5742, -1.6648,\n",
      "         0.6768, -2.7340, -2.9575, -1.6464,  0.4419, -2.6322,  2.8863,  1.7178,\n",
      "         1.7875,  0.0331], requires_grad=True)\n",
      "\n",
      "########## run 2 ##########\n",
      "\n",
      "### Epoch 0 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.701\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 45.99\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 46.03\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.701\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 45.87\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 45.86\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.705\n",
      "Accuracy train: 0.344\n",
      "Time per batch_iter 45.85\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.685\n",
      "Time 4.53\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.694\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 45.98\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 45.93\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 46.03\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 46.01\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 45.78\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.740\n",
      "Time 4.54\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 46.01\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.706\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 46.21\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 45.88\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.695\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 46.08\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.704\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 45.80\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.720\n",
      "Time 4.53\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 45.84\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.699\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 46.04\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.700\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 45.87\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 45.98\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 45.92\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.710\n",
      "Time 4.53\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 46.10\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.90\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.59\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.696\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.79\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.695\n",
      "Time 4.36\n",
      "#####################\n",
      "\n",
      "### Epoch 1 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.703\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.697\n",
      "Accuracy train: 0.406\n",
      "Time per batch_iter 42.82\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.698\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.660\n",
      "Time 4.34\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.438\n",
      "Time per batch_iter 42.77\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.691\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 42.58\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.701\n",
      "Accuracy train: 0.469\n",
      "Time per batch_iter 42.59\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.640\n",
      "Time 4.37\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.79\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.84\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.55\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.692\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.66\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.650\n",
      "Time 4.38\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.375\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.57\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.80\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.60\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.686\n",
      "Accuracy test: 0.630\n",
      "Time 4.35\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.693\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.67\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.59\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.62\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.686\n",
      "Accuracy test: 0.665\n",
      "Time 4.36\n",
      "#####################\n",
      "\n",
      "### Epoch 2 ###\n",
      "\n",
      "Batch iter/total 0/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "Batch iter/total 1/24\n",
      "Loss train: 0.690\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.75\n",
      "\n",
      "Batch iter/total 2/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 3/24\n",
      "Loss train: 0.683\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.70\n",
      "\n",
      "Batch iter/total 4/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.56\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.665\n",
      "Time 4.35\n",
      "#####################\n",
      "\n",
      "Batch iter/total 5/24\n",
      "Loss train: 0.681\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 6/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.719\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 7/24\n",
      "Loss train: 0.682\n",
      "Accuracy train: 0.656\n",
      "Time per batch_iter 42.67\n",
      "\n",
      "Batch iter/total 8/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.77\n",
      "\n",
      "Batch iter/total 9/24\n",
      "Loss train: 0.685\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.72\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.685\n",
      "Accuracy test: 0.670\n",
      "Time 4.35\n",
      "#####################\n",
      "\n",
      "Batch iter/total 10/24\n",
      "Loss train: 0.678\n",
      "Accuracy train: 0.750\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 11/24\n",
      "Loss train: 0.687\n",
      "Accuracy train: 0.500\n",
      "Time per batch_iter 42.71\n",
      "\n",
      "Batch iter/total 12/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.688\n",
      "Time per batch_iter 42.64\n",
      "\n",
      "Batch iter/total 13/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.74\n",
      "\n",
      "Batch iter/total 14/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.63\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.684\n",
      "Accuracy test: 0.690\n",
      "Time 4.35\n",
      "#####################\n",
      "\n",
      "Batch iter/total 15/24\n",
      "Loss train: 0.688\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.85\n",
      "\n",
      "Batch iter/total 16/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.81\n",
      "\n",
      "Batch iter/total 17/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.531\n",
      "Time per batch_iter 42.57\n",
      "\n",
      "Batch iter/total 18/24\n",
      "Loss train: 0.680\n",
      "Accuracy train: 0.625\n",
      "Time per batch_iter 42.67\n",
      "\n",
      "Batch iter/total 19/24\n",
      "Loss train: 0.689\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.58\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.683\n",
      "Accuracy test: 0.715\n",
      "Time 4.36\n",
      "#####################\n",
      "\n",
      "Batch iter/total 20/24\n",
      "Loss train: 0.684\n",
      "Accuracy train: 0.562\n",
      "Time per batch_iter 42.73\n",
      "\n",
      "Batch iter/total 21/24\n",
      "Loss train: 0.674\n",
      "Accuracy train: 0.844\n",
      "Time per batch_iter 42.85\n",
      "\n",
      "Batch iter/total 22/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.54\n",
      "\n",
      "Batch iter/total 23/24\n",
      "Loss train: 0.675\n",
      "Accuracy train: 0.781\n",
      "Time per batch_iter 42.68\n",
      "\n",
      "Batch iter/total 24/24\n",
      "Loss train: 0.686\n",
      "Accuracy train: 0.594\n",
      "Time per batch_iter 42.57\n",
      "\n",
      "### Eval test set ###\n",
      "Loss test: 0.682\n",
      "Accuracy test: 0.710\n",
      "Time 4.37\n",
      "#####################\n",
      "\n",
      "total time taken 10130.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 3\n",
    "num_epochs = 3\n",
    "n_qubits = 9\n",
    "\n",
    "for q_depth in [1, 3, 5]:\n",
    "\n",
    "    q_avg_acc_train, q_avg_acc_test, q_avg_loss_train, q_avg_loss_test = run_experiment(num_runs, num_epochs, nn_type = 'amplitude', q_depth = q_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335dcab",
   "metadata": {},
   "source": [
    "### Plot amplitude embedding metric development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375db7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for q_depth in [1, 3 , 5]:\n",
    "    \n",
    "    model_name=f\"resnet_q_amplitude_depth{q_depth}\"\n",
    "    \n",
    "    # load quantum metrics\n",
    "    q_avg_acc_test = np.load(f'./metrics/{model_name}/avg_acc_test.npy')\n",
    "    q_avg_acc_train = np.load(f'./metrics/{model_name}/avg_acc_train.npy')\n",
    "\n",
    "    q_avg_loss_train = np.load(f'./metrics/{model_name}/avg_loss_train.npy')\n",
    "    q_avg_loss_test = np.load(f'./metrics/{model_name}/avg_loss_test.npy')\n",
    "    \n",
    "    print(f'#### DEPTH {q_depth} ####')\n",
    "    \n",
    "    print(f'max average test accuracy {max(q_avg_acc_test.mean(axis=0)):.3f}')\n",
    "\n",
    "    plot_stuff(q_avg_acc_test, title = 'Acc test',plot_each_metric=True)\n",
    "    plot_stuff(q_avg_loss_test, title = 'Loss test',plot_each_metric=True)\n",
    "    \n",
    "    plot_stuff(q_avg_acc_train, title = 'Acc train',plot_each_metric=True)\n",
    "    plot_stuff(q_avg_loss_train, title = 'Loss train',plot_each_metric=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f4727",
   "metadata": {},
   "source": [
    "### Plot amplitude vs classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb42f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load classic metrics \n",
    "avg_acc_test = np.load('./metrics/resnet/avg_acc_test.npy')\n",
    "avg_acc_train = np.load('./metrics/resnet/avg_acc_train.npy')\n",
    "\n",
    "avg_loss_train = np.load('./metrics/resnet/avg_loss_train.npy')\n",
    "avg_loss_test = np.load('./metrics/resnet/avg_loss_test.npy')\n",
    "\n",
    "acc_test_list = []\n",
    "acc_train_list = []\n",
    "\n",
    "loss_test_list = []\n",
    "loss_train_list = []\n",
    "\n",
    "labels_list = []\n",
    "\n",
    "acc_test_list.append(avg_acc_test)\n",
    "acc_train_list.append(avg_acc_train)\n",
    "\n",
    "loss_test_list.append(avg_loss_test)\n",
    "loss_train_list.append(avg_loss_train)\n",
    "\n",
    "labels_list.append(\"Classic\")\n",
    "\n",
    "\n",
    "for q_depth in [1, 3]:\n",
    "    \n",
    "    model_name=f\"resnet_q_amplitude_depth{q_depth}\"\n",
    "    \n",
    "    print(f'########## {model_name} ##########')\n",
    "\n",
    "    # load quantum metrics\n",
    "    avg_acc_test = np.load(f'./metrics/{model_name}/avg_acc_test.npy')\n",
    "    avg_acc_train = np.load(f'./metrics/{model_name}/avg_acc_train.npy')\n",
    "\n",
    "    avg_loss_train = np.load(f'./metrics/{model_name}/avg_loss_train.npy')\n",
    "    avg_loss_test = np.load(f'./metrics/{model_name}/avg_loss_test.npy')\n",
    "    \n",
    "    acc_test_list.append(avg_acc_test)\n",
    "    acc_train_list.append(avg_acc_train)\n",
    "\n",
    "    loss_test_list.append(avg_loss_test)\n",
    "    loss_train_list.append(avg_loss_train)\n",
    "    \n",
    "    labels_list.append(f\"Q_Resnet_depth_{q_depth}\")\n",
    "\n",
    "# plot in one plot\n",
    "plot_stuff_combined(acc_test_list, labels_list, title = 'Acc test')\n",
    "plot_stuff_combined(loss_test_list, labels_list, title = 'Loss test')\n",
    "\n",
    "plot_stuff_combined(acc_train_list, labels_list, title = 'Acc train')\n",
    "plot_stuff_combined(loss_train_list, labels_list, title = 'Loss train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2816a8",
   "metadata": {},
   "source": [
    "# Backlog "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45a5ab",
   "metadata": {},
   "source": [
    "## Timing experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47468355",
   "metadata": {},
   "source": [
    "### evaluate forward backward passes of example circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = 25\n",
    "\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "\n",
    "dev_remote = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=device_arn,\n",
    "    wires=n_wires,\n",
    "    s3_destination_folder=s3_folder,\n",
    "    parallel=True,\n",
    ")\n",
    "\n",
    "dev_local = qml.device(\"default.qubit\", wires=n_wires)\n",
    "dev_local_braket = qml.device(\"braket.local.qubit\", wires=n_wires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit(params):\n",
    "    for i in range(n_wires):\n",
    "        qml.RX(params[i], wires=i)\n",
    "    for i in range(n_wires):\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
    "    return qml.expval(qml.PauliZ(n_wires - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnode_remote = qml.QNode(circuit, dev_remote)\n",
    "qnode_local = qml.QNode(circuit, dev_local)\n",
    "qnode_local_braket = qml.QNode(circuit, dev_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd3a14",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_wires = 25\n",
    "params = np.random.random(n_wires)\n",
    "\n",
    "t_0_remote = time.time()\n",
    "qnode_remote(params)\n",
    "t_1_remote = time.time()\n",
    "\n",
    "t_0_local = time.time()\n",
    "qnode_local(params)\n",
    "t_1_local = time.time()\n",
    "\n",
    "t_0_local_braket = time.time()\n",
    "qnode_local_braket(params)\n",
    "t_1_local_braket = time.time()\n",
    "\n",
    "print(\"Execution time on remote device (seconds):\", t_1_remote - t_0_remote)\n",
    "print(\"Execution time on local device (seconds):\", t_1_local - t_0_local)\n",
    "print(\"Execution time on local braket device (seconds):\", t_1_local_braket - t_0_local_braket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff846e10",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_qnode_remote = qml.grad(qnode_remote)\n",
    "\n",
    "t_0_remote_grad = time.time()\n",
    "d_qnode_remote(params)\n",
    "t_1_remote_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on remote device (seconds):\", t_1_remote_grad - t_0_remote_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf6eda",
   "metadata": {},
   "source": [
    "### evaluate quantum net with amplitude embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_net_amplitude(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape weights\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    \n",
    "    amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "q_depth = 1\n",
    "n_wires = 9\n",
    "\n",
    "qnode_remote = qml.QNode(quantum_net_amplitude, dev_remote)\n",
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local)\n",
    "qnode_local_braket = qml.QNode(quantum_net_amplitude, dev_local_braket)\n",
    "\n",
    "params = np.random.random(n_wires,requires_grad = True)\n",
    "\n",
    "q_input_features =  np.random.random(512,requires_grad=False)\n",
    "\n",
    "result = qnode_remote(q_weights_flat, q_input_features)\n",
    "\n",
    "print(qnode_remote.draw())\n",
    "\n",
    "result = qnode_local(q_weights_flat, q_input_features)\n",
    "\n",
    "print(qnode_local.draw())\n",
    "\n",
    "result = qnode_local_braket(q_weights_flat, q_input_features)\n",
    "\n",
    "print(qnode_local_braket.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8bf74",
   "metadata": {},
   "source": [
    "#### forward pass for quantum net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "t_0_remote = time.time()\n",
    "qnode_remote(params, q_input_features)\n",
    "t_1_remote = time.time()\n",
    "\n",
    "print(\"Execution time on remote device (seconds):\", t_1_remote - t_0_remote)\n",
    "\n",
    "t_0_local = time.time()\n",
    "qnode_local(params, q_input_features)\n",
    "t_1_local = time.time()\n",
    "\n",
    "print(\"Execution time on local device (seconds):\", t_1_local - t_0_local)\n",
    "\n",
    "t_0_local_braket = time.time()\n",
    "qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket = time.time()\n",
    "\n",
    "print(\"Execution time on local_braket device (seconds):\", t_1_local_braket - t_0_local_braket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84bd56",
   "metadata": {},
   "source": [
    "#### gradient calculation for quantum net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f025d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_qnode_remote = qml.grad(qnode_remote)\n",
    "\n",
    "t_0_remote_grad = time.time()\n",
    "d_qnode_remote(params, q_input_features)\n",
    "t_1_remote_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on remote device (seconds):\", t_1_remote_grad - t_0_remote_grad)\n",
    "\n",
    "d_qnode_local = qml.grad(qnode_local)\n",
    "\n",
    "t_0_local_grad = time.time()\n",
    "d_qnode_local(params, q_input_features)\n",
    "t_1_local_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local device (seconds):\", t_1_local_grad - t_0_local_grad)\n",
    "\n",
    "d_qnode_local_braket = qml.grad(qnode_local_braket)\n",
    "\n",
    "t_0_local_braket_grad = time.time()\n",
    "d_qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket__grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local braket device (seconds):\", t_1_local_braket__grad - t_0_local_braket_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d80afa",
   "metadata": {},
   "source": [
    "### quantum net with hadamard instead of amplitude \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_net_amplitude(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    \n",
    "\n",
    "#     amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "#     # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "    \n",
    "    # Sequence of trainable variational layers\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "        RY_layer(q_weights[k])\n",
    "        \n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_depth = 1\n",
    "n_wires = 9\n",
    "\n",
    "qnode_remote = qml.QNode(quantum_net_amplitude, dev_remote,)\n",
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local)\n",
    "qnode_local_braket = qml.QNode(quantum_net_amplitude, dev_local_braket)\n",
    "\n",
    "\n",
    "params = np.random.randn(n_wires, requires_grad=True)\n",
    "q_input_features = np.random.randn(512, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t_0_remote = time.time()\n",
    "qnode_remote(params, q_input_features)\n",
    "t_1_remote = time.time()\n",
    "\n",
    "print(\"Execution time on remote device (seconds):\", t_1_remote - t_0_remote)\n",
    "\n",
    "t_0_local = time.time()\n",
    "qnode_local(params, q_input_features)\n",
    "t_1_local = time.time()\n",
    "\n",
    "print(\"Execution time on local device (seconds):\", t_1_local - t_0_local)\n",
    "\n",
    "t_0_local_braket = time.time()\n",
    "qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket = time.time()\n",
    "\n",
    "print(\"Execution time on local_braket device (seconds):\", t_1_local_braket - t_0_local_braket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_qnode_remote = qml.grad(qnode_remote)\n",
    "\n",
    "t_0_remote_grad = time.time()\n",
    "d_qnode_remote(params, q_input_features)\n",
    "t_1_remote_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on remote device (seconds):\", t_1_remote_grad - t_0_remote_grad)\n",
    "\n",
    "d_qnode_local = qml.grad(qnode_local)\n",
    "\n",
    "t_0_local_grad = time.time()\n",
    "d_qnode_local(params, q_input_features)\n",
    "t_1_local_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local device (seconds):\", t_1_local_grad - t_0_local_grad)\n",
    "\n",
    "d_qnode_local_braket = qml.grad(qnode_local_braket)\n",
    "\n",
    "t_0_local_braket_grad = time.time()\n",
    "d_qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket__grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local braket device (seconds):\", t_1_local_braket__grad - t_0_local_braket_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccd15c",
   "metadata": {},
   "source": [
    "### forward pass with pytorch interface und hadamard gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a992104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def quantum_net_amplitude(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    \n",
    "\n",
    "#     amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "#     # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "    \n",
    "    # Sequence of trainable variational layers\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "        RY_layer(q_weights[k])\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0)) \n",
    "\n",
    "\n",
    "qnode_remote = qml.QNode(quantum_net_amplitude, dev_remote, interface='torch')\n",
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local, interface='torch')\n",
    "qnode_local_braket = qml.QNode(quantum_net_amplitude, dev_local_braket, interface='torch')\n",
    "\n",
    "n_wires = 9\n",
    "                               \n",
    "params = torch.randn(n_wires, requires_grad=True)\n",
    "q_input_features = torch.randn(512, requires_grad=False)\n",
    "                               \n",
    "import time\n",
    "\n",
    "########## FORWARD ##############\n",
    "\n",
    "t_0_remote = time.time()\n",
    "result_remote = qnode_remote(params, q_input_features)\n",
    "t_1_remote = time.time()\n",
    "\n",
    "print(\"Execution time on remote device (seconds):\", t_1_remote - t_0_remote)\n",
    "\n",
    "t_0_local = time.time()\n",
    "result_local = qnode_local(params, q_input_features)\n",
    "t_1_local = time.time()\n",
    "\n",
    "print(\"Execution time on local device (seconds):\", t_1_local - t_0_local)\n",
    "\n",
    "t_0_local_braket = time.time()\n",
    "result_local_braket = qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket = time.time()\n",
    "\n",
    "print(\"Execution time on local_braket device (seconds):\", t_1_local_braket - t_0_local_braket)\n",
    "\n",
    "print()\n",
    "########## BACKWARD ###########\n",
    "\n",
    "t_0_remote_grad = time.time()\n",
    "result_remote.backward()\n",
    "t_1_remote_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on remote device (seconds):\", t_1_remote_grad - t_0_remote_grad)\n",
    "                               \n",
    "                               \n",
    "t_0_local_grad = time.time()                               \n",
    "result_local.backward()  \n",
    "t_1_local_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local device (seconds):\", t_1_local_grad - t_0_local_grad)                               \n",
    "\n",
    "t_0_local_braket_grad = time.time()                               \n",
    "result_local_braket.backward()    \n",
    "t_1_local_braket__grad = time.time()                               \n",
    "print(\"Gradient calculation time on local braket device (seconds):\", t_1_local_braket__grad - t_0_local_braket_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d1ba1",
   "metadata": {},
   "source": [
    "### forward pass with pytorch interface und amplitude embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def quantum_net_amplitude(q_weights_flat, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    \n",
    "\n",
    "    amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "#     # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "#     H_layer(n_qubits)\n",
    "    \n",
    "    # Sequence of trainable variational layers\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "        RY_layer(q_weights[k])\n",
    "        \n",
    "    return qml.expval(qml.PauliZ(0)) \n",
    "\n",
    "\n",
    "qnode_remote = qml.QNode(quantum_net_amplitude, dev_remote, interface='torch')\n",
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local, interface='torch')\n",
    "qnode_local_braket = qml.QNode(quantum_net_amplitude, dev_local_braket, interface='torch')\n",
    "\n",
    "n_wires = 9\n",
    "                               \n",
    "params = torch.randn(n_wires, requires_grad=True)\n",
    "q_input_features = torch.randn(512, requires_grad=False)\n",
    "                               \n",
    "import time\n",
    "\n",
    "########## FORWARD ##############\n",
    "\n",
    "t_0_remote = time.time()\n",
    "result_remote = qnode_remote(params, q_input_features)\n",
    "t_1_remote = time.time()\n",
    "\n",
    "print(\"Execution time on remote device (seconds):\", t_1_remote - t_0_remote)\n",
    "\n",
    "t_0_local = time.time()\n",
    "result_local = qnode_local(params, q_input_features)\n",
    "t_1_local = time.time()\n",
    "\n",
    "print(\"Execution time on local device (seconds):\", t_1_local - t_0_local)\n",
    "\n",
    "t_0_local_braket = time.time()\n",
    "result_local_braket = qnode_local_braket(params, q_input_features)\n",
    "t_1_local_braket = time.time()\n",
    "\n",
    "print(\"Execution time on local_braket device (seconds):\", t_1_local_braket - t_0_local_braket)\n",
    "\n",
    "print()\n",
    "########## BACKWARD ###########\n",
    "\n",
    "t_0_remote_grad = time.time()\n",
    "result_remote.backward()\n",
    "t_1_remote_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on remote device (seconds):\", t_1_remote_grad - t_0_remote_grad)\n",
    "                               \n",
    "                               \n",
    "t_0_local_grad = time.time()                               \n",
    "result_local.backward()  \n",
    "t_1_local_grad = time.time()\n",
    "\n",
    "print(\"Gradient calculation time on local device (seconds):\", t_1_local_grad - t_0_local_grad)                               \n",
    "\n",
    "t_0_local_braket_grad = time.time()                               \n",
    "result_local_braket.backward()    \n",
    "t_1_local_braket__grad = time.time()                               \n",
    "print(\"Gradient calculation time on local braket device (seconds):\", t_1_local_braket__grad - t_0_local_braket_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd423185",
   "metadata": {},
   "source": [
    "### check different differentiation methods for pytorch with amplitude embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blubb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95458cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af10c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wires = 2\n",
    "n_qubits = 2\n",
    "q_depth = 1\n",
    "\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def entangling_layer(n_qubits = n_qubits, pattern = 'chain'):\n",
    "    \"\"\"pattern in ('chain', 'ring', 'all_to_all')\"\"\"\n",
    "    \n",
    "    if pattern not in ('chain', 'ring', 'all_to_all', 'random'):\n",
    "        pattern = 'chain'\n",
    "    if pattern is not 'random':\n",
    "        qml.broadcast(qml.CNOT, wires = list(range(n_qubits)), pattern = pattern)\n",
    "    else:\n",
    "        q_idx_list = list(range(n_qubits))\n",
    "        np.random.shuffle(q_idx_list)\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires = [q_idx_list[i], q_idx_list[i + 1]])\n",
    "        qml.CNOT(wires = [q_idx_list[-1], q_idx_list[0]])\n",
    "        \n",
    "q_input_features = torch.randn(2**n_qubits , requires_grad=False)*2        \n",
    "\n",
    "def quantum_net_amplitude(q_weights_flat):#, q_input_features=None):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit with amplitude embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    \n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits, 2)  \n",
    "    \n",
    "    amplitude_embedding(q_input_features, n_qubits)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for k in range(q_depth):\n",
    "        RY_layer(q_weights[k,:,0])\n",
    "        RX_layer(q_weights[k,:,1])\n",
    "        entangling_layer(n_qubits = n_qubits, pattern = 'ring')\n",
    "\n",
    "#     return tuple([qml.expval(qml.PauliZ(idx)) for idx in range(n_qubits)])\n",
    "    return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "\n",
    "\n",
    "class Blubb(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    " \n",
    "        self.q_params = nn.Parameter(np.pi / 2 * (torch.rand(n_wires * q_depth * 2, requires_grad=True) * 2 - 1))\n",
    "        self.qnode_local = qml.QNode(quantum_net_amplitude, dev_local, interface='torch')\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return self.qnode_local(self.q_params)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "\n",
    "dev_remote = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=device_arn,\n",
    "    wires=n_wires,\n",
    "    s3_destination_folder=s3_folder,\n",
    "    parallel=True,\n",
    ")\n",
    "\n",
    "dev_local_braket = qml.device(\"braket.local.qubit\", wires=n_wires)\n",
    "dev_local = qml.device(\"default.qubit\", wires=n_wires)\n",
    "\n",
    "diff_method = [\"best\"]\n",
    "    \n",
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local, interface='torch')\n",
    "\n",
    "params = np.pi / 2 * (torch.rand(n_wires * q_depth *2 , requires_grad=True) * 2 - 1)\n",
    "test = qnode_local(params)\n",
    "\n",
    "import time\n",
    "print(qnode_local.draw())\n",
    "\n",
    "########## FORWARD ##############\n",
    "\n",
    "blubb = Blubb()\n",
    "\n",
    "optimizer = optim.Adam(blubb.parameters(),lr=0.1)\n",
    "result_list = []\n",
    "for iter_ in range(100):\n",
    "    \n",
    "    print(f'Iteration {iter_}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    t_0_local = time.time()\n",
    "    \n",
    "    result_local = blubb()\n",
    "    result_list.append(result_local)\n",
    "    \n",
    "    t_1_local = time.time()\n",
    "\n",
    "    print(result_local.item())\n",
    "\n",
    "    t_0_local_grad = time.time()                               \n",
    "    \n",
    "    result_local.backward() \n",
    "    \n",
    "    print(f'Gradient {blubb.q_params.grad}')\n",
    "    \n",
    "    t_1_local_grad = time.time()\n",
    "\n",
    "    optimizer.step()\n",
    "    print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnode_local = qml.QNode(quantum_net_amplitude, dev_local, interface='torch')\n",
    "\n",
    "params = np.pi/2*(torch.rand(n_wires * q_depth *2 , requires_grad=True)*2-1)\n",
    "test = qnode_local(blubb.q_params)\n",
    "import time\n",
    "print(qnode_local.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([-1.0893, -0.2871,  0.7455,  0.9715]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
